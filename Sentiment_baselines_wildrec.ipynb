{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vCSmAAFVX1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fda846f-f190-4597-ea7a-72e279151f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "drive.mount('/content/drive/')\n",
        "import string\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/others/Data/Validation\n",
        "! ls drive/MyDrive/others/Test/bangla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWdYx6pqzqND",
        "outputId": "b8556177-ae98-46e1-f88d-93168a7a7f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid_Bangla.csv  valid_Hindi.csv  valid_Magahi.csv\n",
            "test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading And Preprocessing"
      ],
      "metadata": {
        "id": "NqrTpZAYxqNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('drive/MyDrive/others/Data/Training/train_Bangla.csv')\n",
        "train_df['sentence'] = train_df['sentence'].str.lower()\n",
        "valid_df = pd.read_csv('drive/MyDrive/others/Data/Validation/valid_Bangla.csv')\n",
        "valid_df['sentence'] = valid_df['sentence'].str.lower()\n",
        "test_df = pd.read_csv('drive/MyDrive/others/Test/bangla/test.csv')\n",
        "test_df['sentence'] = valid_df['sentence'].str.lower()\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "\n",
        "punct = string.punctuation\n",
        "\n",
        "def remove_punct_one(txt):\n",
        "  for c in list(punct):\n",
        "    txt = txt.replace(c, \"\")\n",
        "  return txt\n",
        "\n",
        "train_df['sentence'] = train_df['sentence'].apply(remove_punct_one)\n",
        "valid_df['sentence'] = valid_df['sentence'].apply(remove_punct_one)\n",
        "test_df['sentence'] = test_df['sentence'].apply(remove_punct_one)"
      ],
      "metadata": {
        "id": "m0hJ6bP20Yet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocab implementation on pre-processed dataset"
      ],
      "metadata": {
        "id": "7hDWqtLRxwTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets_and_vocab(data_frame):\n",
        "    ## all the files for MEMD-ABSA\n",
        "    sentences = list(map(lambda x: tokenizer.tokenize(x), data_frame['sentence']))\n",
        "    vocab = build_vocab_from_iterator(sentences, specials=[\"<unk>\"])\n",
        "    vocab.set_default_index(vocab['<unk>'])\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "-2J3t3Q9y7Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = prepare_datasets_and_vocab(train_df)\n",
        "print(f\"Unique Words: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MRuj3x42Zsb",
        "outputId": "6b725f6b-5de0-4935-d9c0-21935fa0ccb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Words: 14287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Senitment Labels:')\n",
        "train_df['sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYbMSqjGlmsk",
        "outputId": "2798e25d-b8d2-4f49-e4e4-f5e43bb6199b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Senitment Labels:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive    293\n",
              "Negative    247\n",
              "Neutral     163\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "Embedding size: 100\n",
        "\n",
        "BI-LSTM: 64*2\n",
        "\n",
        "Fully Connected: 128\n",
        "\n",
        "Fully Connected: 3"
      ],
      "metadata": {
        "id": "kCpjswDRx3pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        '''double embedding + lstm encoder + dot self attention'''\n",
        "        super(SentimentModel, self).__init__()\n",
        "\n",
        "        self.gen_embedding = torch.nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "        # self.dropout1 = torch.nn.Dropout(0.5)\n",
        "        # self.dropout2 = torch.nn.Dropout(0)\n",
        "\n",
        "        ## input size 400, output size 2 x 50\n",
        "        self.bilstm = torch.nn.LSTM(emb_size, 64,\n",
        "                                    num_layers=3, batch_first=True, bidirectional=True)\n",
        "        self.fc1 = torch.nn.Linear(128, 64)\n",
        "\n",
        "        self.class_layer = torch.nn.Linear(64, 3)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        emb = self.gen_embedding(X)\n",
        "\n",
        "        output, (h_n, c_n) =  self.bilstm(emb)\n",
        "        # print(output.shape)\n",
        "        # fc1 = self.fc1(F.relu(output[:, -1, :]))\n",
        "        fc1 = self.fc1(output[:, -1, :])\n",
        "        classout = self.class_layer(fc1)\n",
        "        return classout\n",
        "\n"
      ],
      "metadata": {
        "id": "CQAOueVO3OXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentDataset(Dataset):\n",
        "  def __init__(self, data_df, tokenizer, vocab, maxlen=128):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.vocab = vocab\n",
        "    self.data_df = data_df\n",
        "    self.maxlen = maxlen\n",
        "    self.label_encoding = { # add more if more sentiment labels are present\n",
        "        \"Negative\": 0,\n",
        "        \"Neutral\": 1,\n",
        "        \"Positive\":2,\n",
        "    }\n",
        "    self.punct = string.punctuation\n",
        "    self.encoded_data = []\n",
        "    self._build()\n",
        "\n",
        "  def _build(self):\n",
        "    for sentence, label in self.data_df.values:\n",
        "      enc_tokens = [0] * self.maxlen\n",
        "      tokens = self.tokenizer.tokenize(sentence)\n",
        "      if len(tokens)<=128:\n",
        "        enc_tokens[:len(tokens)] = self.vocab(tokens)\n",
        "      else:\n",
        "        enc_tokens[:128] = self.vocab(tokens)[:128]\n",
        "      lab = self.label_encoding[label]\n",
        "      self.encoded_data.append([torch.tensor(enc_tokens),  torch.tensor(lab)])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "     return self.encoded_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.encoded_data)\n"
      ],
      "metadata": {
        "id": "Dy4xDj0X3-b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentDataset(train_df, tokenizer, vocab)\n",
        "valid_dataset = SentDataset(valid_df, tokenizer, vocab)\n",
        "test_dataset = SentDataset(test_df, tokenizer, vocab)"
      ],
      "metadata": {
        "id": "pR3CqZTcrc3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32)\n",
        "model = SentimentModel(len(vocab), 150)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "best_model = None\n"
      ],
      "metadata": {
        "id": "fw3sJJfh_tSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loop(pred_model, dataloader):\n",
        "  valid_ite = tqdm(dataloader)\n",
        "  preds = []\n",
        "  golds = []\n",
        "  pred_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_ite:\n",
        "      x, y = batch\n",
        "      pred_out = pred_model(x)\n",
        "      preds += pred_out.argmax(dim=-1).tolist()\n",
        "      golds += y.tolist()\n",
        "    precise_macro = precision_score(golds, preds, average='macro', zero_division=0)\n",
        "    recall_macro = recall_score(golds, preds, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(golds, preds, average='macro', zero_division=0)\n",
        "\n",
        "  return {\n",
        "      'macro_precision': precise_macro,\n",
        "      'macro_recall': recall_macro,\n",
        "      'macro_f1': f1_macro\n",
        "  }"
      ],
      "metadata": {
        "id": "Te6Qq8UsulA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 64\n",
        "best_f1 = -1\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_epoch_loss = 0\n",
        "  model.train()\n",
        "  train_ite = tqdm(train_dataloader)\n",
        "  for batch in train_ite:\n",
        "    x, l = batch\n",
        "    output = model(x)\n",
        "    loss = loss_fn(output, l)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_ite.set_postfix({'training loss': loss.item()})\n",
        "    train_epoch_loss += loss.item()\n",
        "\n",
        "  validation_epoch_loss = 0\n",
        "  valid_ite = tqdm(valid_dataloader)\n",
        "  preds = []\n",
        "  golds = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_ite:\n",
        "      x, y = batch\n",
        "      pred_out = model(x)\n",
        "      preds += pred_out.argmax(dim=-1).tolist()\n",
        "      golds += y.tolist()\n",
        "      val_loss = loss_fn(pred_out,y)\n",
        "      valid_ite.set_postfix({'validation loss': val_loss.item()})\n",
        "      validation_epoch_loss = val_loss.item()\n",
        "    print()\n",
        "    print('--'*20, ' Validation Scores ', '--'*20)\n",
        "    precise_macro = precision_score(golds, preds, average='macro', zero_division=0)\n",
        "    recall_macro = recall_score(golds, preds, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(golds, preds, average='macro', zero_division=0)\n",
        "    if f1_macro > best_f1:\n",
        "      best_model = copy.deepcopy(model)\n",
        "      best_f1 = f1_macro\n",
        "    print(f'EPOCH: {epoch}')\n",
        "    print(f'avg training loss: {(train_epoch_loss/len(train_dataset))}, avg validation loss:{(validation_epoch_loss/len(valid_dataset))}')\n",
        "    print(f'precision:{precise_macro}, recall:{recall_macro}, f1:{f1_macro}')\n",
        "    # if best_f1<f1_macro:\n",
        "    # print('--'*50)\n",
        "\n"
      ],
      "metadata": {
        "id": "JCm3d8HiRnHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING"
      ],
      "metadata": {
        "id": "8sNZqsibxknq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "eval_loop(best_model, test_dataloader)"
      ],
      "metadata": {
        "id": "sLO16WuxT1La",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808609ec-bbb8-4de3-9ec4-bd0871d9df40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 19.11it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'macro_precision': 0.3600362823831331,\n",
              " 'macro_recall': 0.3631226914245782,\n",
              " 'macro_f1': 0.348537452885279}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}